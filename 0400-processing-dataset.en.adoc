[[processing]]
== Processing a dataset

If you have prepared a dataset in one of the suggested templates/formats, and used the MDT before or used the two quick start guides, you should be able to upload and process a dataset by following the steps below.

For specific usage or error messages refer to the detailed user guide below.

=== Log in

. Go to the website of https://edna-tool.gbif-uat.org/[the MDT^].
. Log in.
. Press *New Dataset* in the upper part of the page.

=== Upload data (step 1)

. Drag your dataset to the upload area, or click and select the file.
. Give the dataset a nickname (e.g. "my_first_dataset")
. Press *Start Upload*.
+
A green icon will indicate if a recogniced format is detected and looks OK according to some very basic data checks.

image::img/advanced_example_upload_warning.png[]

. (_optional_) Inspect your data in the data viewer by clicking on the eye icon next to the uploaded dataset
+ 
. Press *Proceed*

=== Map terms (step 2)

On this page you tell the MDT which standardised field names (Darwin Core terms) correspond to which fields in your data.

TIP: Press *how to use this form* to get a guided tour of this page.

TIP: Press *Save mapping* once in a while to make sure that you will not be logged out and lose your work.

*First inspection*

. Inspect the overall structure and information on the page.
.. The upper section named *Sample* maps your sample data fields to Darwin Core terms (first column), automatically identifying and mapping fields from the *Samples* sheet (second column) and global fields from the *Study* sheet (third column) with their identically named Darwin Core counterparts.
.. The second section named *Taxon* does the same for taxonomic and sequence related information, auto-mapping fields from the Taxon sheet to identically named Darwin Core fields.
... If you uploaded sequences in a separate fasta file *Seqs.fasta*, this will be indicated in the mapping of term:dwc[DNA_sequences].
.. The last section *Unmapped fields* lists all the fields in your data, that has names which the tool did not automatically  recognize. Below there is an option to put unmapped fields into *Extended Measurement Or Facts*.

*Completing the mapping*

Starting from the top with *Sample* and *Taxonomy* information

. make sure that the automatically mapped fields are correct – both fields from the *Samples* table and from the *Study* table.
. take a look a the *Unmapped fields* and check if there are any fields in the uploaded data that have not been mapped.
.. For each unmapped field where you expect (or know) that there is a suitable Darwin Core term in either the *Sample* or *Taxonomy* section:
... Click on *Add mapping for another sample field* and inspect and search the list of available terms.
... Add the term, and select the field from the uploaded data that you wish to map.
... If you cannot find a relevant term, you may want to look at the official resources with available terms from https://rs.gbif.org/core/dwc_occurrence_2024-02-23.xml[Darwin Core Occurrence] and https://rs.gbif.org/extension/gbif/1.0/dna_derived_data_2024-04-17.xml[dna-derived data].

If you still have *Unmapped fields*, that you wish to include in the dataset to be published to GBIF, you may consider accomodating those in https://rs.gbif.org/extension/obis/extended_measurement_or_fact_2023-08-28.xml[Extended Measurement Or Facts] (eMoF) in this way:

. Click on the "unmapped" field you wish to put into eMoF from the row of unmapped fields to transfer it to the the eMoF section below as a new entry.
. Add a measurement unit and other relevant information to the field, if possible.

Now, the mapping is complete.

NOTE: All available standard fields (from https://rs.gbif.org/core/dwc_occurrence_2024-02-23.xml[Darwin Core Occurrence] and https://rs.gbif.org/extension/gbif/1.0/dna_derived_data_2024-04-17.xml[dna-derived data]) can be included in the upload files, and require no manual mapping if spelled correctly.

. Press *Proceed*.

=== Process data (step 3)

. Press *Process data*.
+
The tool goes through a series of steps which will be indicated as succesful with a green tick-mark, and finally produces standardized BIOM files, which the tool uses as an intermediate file format.
+
NOTE: If the barcoding region used for your dataset is one of the regions included in the https://www.gbif.org/tools/sequence-id[GBIF Sequence ID tool^], the option *assign taxonomy* will be available. You can use that to assign taxonomy to the OTUs by comparing the sequences with a reference database. NB: This overwrites any taxonomy provided in the data.

. Check that number of samples and taxa are as expected.
. Press *Proceed*

=== Review (step 4)

Here the data can be explored to check that everything is OK. The options in this step are intended as sanity checks of the data to ensure that e.g. negative control samples have been removed, and that the mapping is as expected.

* Check the map and verify that the samples are placed geographically where expected.
* Check the taxonomic barchart to ensure that taxonomic composition is as expected.
* Check ordination plots (PCoA/MDS) – that visualise compositional differnence of the samples – for outliers (any control samples that should have been excluded?).
* Select single samples from the map or from charts and explore their metadata and taxonomy in the panel to the right.
. Press *Proceed*

=== Add metadata (step 5)

On this page, dataset metadata (dataset description, persons and affiliations, etc.) is added in a minimalistic form.

TIP: toggle "Show help" to get guidance text for the fields.

. Add a meaningful title.
. Select a licence.
. Give as rich a dataset description as you can.
. Add all persons you wish to associate with the dataset.
. Be sure to put the person that knows about this dataset (you?) as the contact, so that response and issues about this dataset is directed to the correct person.
. Fill out the other fields as good as possible.
. Press *Proceed*.

=== Export (step 6)

This last page of the process produces a Darwin Core Archive that can be published directly to the https://www.gbif-uat.org/[GBIF test environment (UAT)^] from the MDT. In technical terms this version of the dataset will be hosted and published by the dummy publisher https://www.gbif-uat.org/publisher/f7ecf12b-221d-4eea-806d-fb4b37face25[*GBIF eDNA Test organisation*] with an endpoint at https://hosted-datasets.gbif-uat.org/edna/.

. Press *Create DWC archive*.
+
This creates the <<dwc-a>> from the data, going through a series of steps, that will be indicated as succesful with a green tick-mark.

. Press *Publish to GBIF test environment (UAT)*.
+
A prompt will inform that it takes some minutes before the data is fully ingested and will show up with all samples in the GBIF test environment (and the map will only appear the next day). A link to the dataset in the test environment will appear next to the *Publish* button.

. Explore the dataset in the test environment
. Ensure that all information and data is processed and displayed appropriately.

WARNING: Currently, the MDT is in the GBIF test environment, and it is still being developed and has not been formally released. Uploaded datasets and the produced BIOM files and Darwin Core Archive should be downloaded and stored locally to ensure they are not lost in case of problems in the GBIF test environment.

NOTE: If you end up with a dataset suitable for proper publication to GBIF.org, go to <<publishing>>.
